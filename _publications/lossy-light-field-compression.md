---
title: "Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques"
collection: publications
permalink: /research/lossy-light-field-compression
link: 'https://zarkonium.github.io/lossy-light-field-compression/'
#excerpt: 'Lossy data compression is a particular type of informational encoding utilizing approximations in order to efficiently tradeoff accuracy in favour of smaller file sizes. The transmission and storage of images is a typical example of this in the modern digital world. However the reconstructed images often suffer from degradation and display observable visual artifacts. Convolutional Neural Networks (CNNs) have garnered much attention in all corners of Computer Vision (CV), including the tasks of image compression and artifact reduction. We study how lossy compression can be extended to higher dimensional images with varying viewpoints, known as light fields. Domain Randomization (DR) is explored in detail, and used to generate the largest light field dataset we are aware of, to be used as training data. We formulate the task of compression under the frameworks of neural networks and calculate a quantization tensor for the 4-D Discrete Cosine Transform (DCT) coefficients of the light fields. In order to accurately train the network, a high degree approximation to the rounding operation is introduced. In addition, we present a multi-resolution convolutional-based light field enhancer, producing average gains of 0.854 db in Peak Signal-to-Noise Ratio (PSNR), and 0.0338 in Structual Similarity Index Measure (SSIM) over the base model, across a wide range of bitrates.'
date: 2022-12-14
venue: 'YorkSpace'
paperurl: 'http://zarkonium.github.io/files/Lossy_Light_Field_Compression_Using_Modern_Deep_Learning_and_Domain_Randomization_Techniques.pdf'
citation: 'Valtchev, S.Z. and Wu, J. (2022). &quot;Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques&quot;, <i>YorkSpace</i>. 2022-12-14.'
---
#### Lossy data compression is a particular type of informational encoding utilizing approximations in order to efficiently tradeoff accuracy in favour of smaller file sizes. The transmission and storage of images is a typical example of this in the modern digital world. However the reconstructed images often suffer from degradation and display observable visual artifacts. Convolutional Neural Networks (CNNs) have garnered much attention in all corners of Computer Vision (CV), including the tasks of image compression and artifact reduction. We study how lossy compression can be extended to higher dimensional images with varying viewpoints, known as light fields. Domain Randomization (DR) is explored in detail, and used to generate the largest light field dataset we are aware of, to be used as training data. We formulate the task of compression under the frameworks of neural networks and calculate a quantization tensor for the 4-D Discrete Cosine Transform (DCT) coefficients of the light fields. In order to accurately train the network, a high degree approximation to the rounding operation is introduced. In addition, we present a multi-resolution convolutional-based light field enhancer, producing average gains of 0.854 db in Peak Signal-to-Noise Ratio (PSNR), and 0.0338 in Structual Similarity Index Measure (SSIM) over the base model, across a wide range of bitrates.

### Bibtex
```
@article{valtchev2022LFCompression,
  title={Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques},
  author={Valtchev, Svetozar Zarko},
  publisher={YorkSpace Institutional Repository},
  year={2022},
  school={York University},
  doi={[https://doi.org/10.1186/s40537-021-00455-5](http://hdl.handle.net/10315/40642)},
}
```
